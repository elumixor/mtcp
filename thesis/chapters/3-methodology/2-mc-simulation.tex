
\section{Simulated Data and Event Weighting}

After formulating our problem as a supervised learning task, the next step is to find an appropriate way to tackle it.
To train a classifier, we need a set of labeled observations. In the context of particle physics, each observation is an
event and the label corresponds to the process that led to this event. However, obtaining these labeled observations is
not as straightforward as simply recording the events.

We need a model that describes the underlying physics. In our case, this is the Standard Model, including the branching
factors introduced before. The model is based on our understanding of particle physics and describes the probabilities
of different processes and outcomes.

To generate our labeled dataset, we use a tool called Monte Carlo (MC) simulations. MC simulations are a way to generate
data that follows a given model. In essence, the simulation creates a large number of events, each following the
probabilistic rules of the model. We can then record these events and their corresponding labels (which process they
were created by).

The benefit of MC simulations is that they allow us to create a large amount of data where we know the 'true' labels.
This data can be used to train and evaluate our classifier. However, it's important to note that while MC simulations
are based on our best understanding of physics, they are not perfect. There are always uncertainties and potential
inaccuracies in the model, which can affect the generated data and the performance of our classifier.

To alleviate the difficulties of training on small dataset, more signal events are explicitly generated. This leads to
producing a different distribution than the one observed in the real data. To align the MC simulated data with the real
data, it is necessary to apply event weights. Event weighting is a technique used to correct the simulated distributions
for known differences between the simulation and the real data.

% Event weights can account for a variety of effects:

% Luminosity: The total number of simulated events often does not correspond to the luminosity of the real data. Events
% are therefore weighted to correspond to the correct integrated luminosity. If $L_{data}$ is the integrated luminosity of
% the data and $L_{MC}$ is the integrated luminosity of the MC sample, then the luminosity weight for an event is $w_{L} =
%     L_{data}/L_{MC}$.

% Cross-Section: Different processes have different probabilities (cross-sections) of occurring. The ratio of the
% cross-sections in data and simulation $\sigma_{data}/\sigma_{MC}$ is used as a weight.

% Detector Effects: The detector response is not always perfectly simulated. Therefore, weights are applied to correct for
% known discrepancies in detector efficiencies and energy resolutions between the data and the simulation.

% Pileup: Pileup refers to additional proton-proton collisions that occur simultaneously with the event of interest.
% Pileup can significantly affect the event reconstruction. Pileup weights are used to match the pileup distribution in
% the data.

% Higher-order corrections: Theoretical predictions often include higher-order corrections, known as K-factors, to account
% for processes beyond the leading-order approximation used in the simulation.

% In mathematical terms, the total weight $w_{total}$ for an event can be written as the product of individual weights:

% \begin{equation}
%     w_{\text{total}} = w_{L} w_{\sigma} w_{\text{detector}} w_{\text{pileup}} w_{\text{K-factor}}
% \end{equation}

% Event weights are essential for ensuring that the simulated events accurately represent the conditions of the actual
% experiment. These weights allow us to make meaningful comparisons between the data and theoretical predictions, and are
% a crucial part of the analysis in high-energy physics.


% \input{chapters/3-data/v8/0-section.tex}


\subsection{Regions in Particle Physics}

In particle physics, especially in high energy experiments such as those conducted at the LHC, the parameter space is
vast. Identifying signals from noise or background events becomes a challenging task. To tackle this challenge, we
divide the parameter space into different regions, each serving a specific purpose. These regions include the Signal
Region (SR), Control Region (CR), and Validation Region (VR).

\begin{itemize}
    \item \textbf{Signal Region (SR)}: The Signal Region is where we expect the events of interest, the
          signal, to be most prevalent. It is defined by certain selection criteria that maximize the signal's prominence against
          the background. In our case, the signal refers to the \tth events.

    \item \textbf{Control Region (CR)}: The Control Region is where we estimate the amount of background contamination
          present in the Signal Region. The CR is characterized by negligible signal but a significant amount of background
          events, similar to what we expect in the SR. By studying the CR, we can understand and model the background in the SR.

    \item \textbf{Validation Region (VR)}: The Validation Region is used to test the reliability of our model predictions
          and the Monte Carlo simulations. The VR is typically chosen where neither the signal nor the background is expected to
          be particularly high or low. Any significant deviation of the observed data from our model predictions in the VR may
          indicate the presence of a new physics process or systematic errors in our model or simulation.  \end{itemize}

These regions are not arbitrarily defined but are carefully chosen based on detailed knowledge of the physics processes
involved and the detector's characteristics. They provide a robust framework for analyzing the data from high-energy
physics experiments and are crucial in our search for the \tth process events.

\subsection{\lss channel}

\input{figures/feynman/2lss1tau}

The work at CERN is often divided into different channels, with each channel focusing on a specific final state of
interest. These channels are orthogonal, meaning that each event can only belong to one channel, preventing any overlap
in the analysis.

One such channel, and the main focus of this thesis, is the two-lepton same-sign plus one tau (\lss) channel. This
specific final state arises from the decay of a top-antitop-Higgs (\tth) system, where one top quark decays to a W boson
and a b quark, with the W boson further decaying to a lepton and a neutrino. The other top quark also decays to a W
boson and a b quark, but in this case, the W boson decays into a pair of quarks. Lastly, the Higgs boson decays to a
pair of tau leptons, where one tau lepton decays into another lepton and two neutrinos, while the other tau lepton
decays to a pair of quarks and a neutrino. This complex series of decays results in a final state consisting of two
same-sign leptons and a hadronically decaying tau lepton, hence the name \lss channel (as shown in \autoref{fig:2lss1tau}).

The branching ratios for these decays are as follows:

\begin{align*}
    \text{Higgs boson to a pair of tau leptons: }\mathcal{B}(H \rightarrow \tau\tau)  & = 6.25\% \\
    \text{tau lepton to a W boson and a neutrino: }\mathcal{B}(\tau \rightarrow W\nu) & = 100\%  \\
    \text{top quark to a W boson and a b quark: }\mathcal{B}(t \rightarrow Wb)        & = 100\%  \\
    \text{W boson to a lepton and a neutrino: }\mathcal{B}(W \rightarrow l\nu)        & = 10\%   \\
    \text{W boson to a pair of quarks: }\mathcal{B}(W \rightarrow qq)                 & = 70\%
\end{align*}

Given that the theoretical cross-section for the \tth process is 500 fb and the luminosity is 140 fb$^{-1}$, the total
number of expected \tth events is approximately 70,000. When accounting for the aforementioned branching ratios, the
expected number of \tth events in the \lss final state is reduced to:

\begin{align*}
    N_{\text{\tth, \lss}} & = \sigma_{\text{\tth}} \cdot \mathcal{L} \cdot \mathcal{B}(H \rightarrow \tau\tau) \cdot \mathcal{B}(\tau \rightarrow W\nu) \cdot \mathcal{B}(t \rightarrow Wb) \cdot \mathcal{B}(W \rightarrow l\nu) \cdot \mathcal{B}(W \rightarrow qq) \\
                          & = 500 \text{ fb} \cdot 140 \text{ fb}^{-1} \cdot 6.25\% \cdot 100\% \cdot 100\% \cdot 10\% \cdot 70\% \approx 3.15
\end{align*}

It should be noted, however, that this is a simplified calculation and does not consider various factors such as
detector efficiencies and background events. Nevertheless, it provides a good estimate of the scale of the challenge we
face in detecting the \tth process in the \lss channel.

\subsection{Distribution of the variables inside \gls{sr}}

The following figures (\autoref{fig:distributions1} and \autoref{fig:distributions2}) show the distributions of some variables
of interest inside \gls{sr}.

\captionsetup[subfigure]{justification=centering}
\begin{figure}[htb!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/plots/histograms/lep_pt_0.png}
        \caption{Distribution of the transverse momentum of the leading lepton.}
        \label{fig:lep_pt_0}
    \end{subfigure}\hfill%
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/plots/histograms/lep_pt_1.png}
        \caption{Distribution of the transverse momentum of the subleading lepton.}
        \label{fig:lep_pt_1}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/plots/histograms/lep_Eta_0.png}
        \caption{Distribution of the pseudorapidity of the leading lepton.}
        \label{fig:lep_Eta_0}
    \end{subfigure}\hfill%
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/plots/histograms/lep_Eta_1.png}
        \caption{Distribution of the pseudorapidity of the subleading lepton.}
        \label{fig:lep_Eta_1}
    \end{subfigure}
    \caption{Distributions of the variables inside \gls{sr} (part 1)}
    \label{fig:distributions1}
\end{figure}

\newpage

\begin{figure}[htb!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/plots/histograms/lep_Phi_0.png}
        \caption{Distribution of the azimuthal angle of the leading lepton.}
        \label{fig:lep_Phi_0}
    \end{subfigure}\hfill%
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/plots/histograms/lep_Phi_1.png}
        \caption{Distribution of the azimuthal angle of the subleading lepton.}
        \label{fig:lep_Phi_1}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/plots/histograms/njets.png}
        \caption{Distribution of the number of jets.}
        \label{fig:njets}
    \end{subfigure}\hfill%
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/plots/histograms/nbjets.png}
        \caption{Distribution of the number of $b$-jets.}
        \label{fig:nbjets}
    \end{subfigure}
    \caption{Distributions of the variables inside \gls{sr} (part 2)}
    \label{fig:distributions2}
\end{figure}

\begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c|c}
        $t\bar{t}H$ & \textbf{v6} & \textbf{v8} &      &       \\
        \hline
        Weighted    & 1000        & 500         & -500 & -50\% \\
        Raw         & 1000        & 500         & -500 & -50\% \\
        \hline
    \end{tabular}
    \captionof{table}{Number of ttH events in the SR for v6 and v8.}
    \label{tab:ttH_event_numbers1}
\end{minipage}\hfill%
\begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c|c}
        \gls{sr} & \textbf{v6} & \textbf{v8} &      &       \\
        \hline
        Weighted & 1000        & 500         & -500 & -50\% \\
        Raw      & 1000        & 500         & -500 & -50\% \\
        \hline
    \end{tabular}
    \captionof{table}{Number of all the events in the SR for v6 and v8.}
    \label{tab:ttH_event_numbers2}
\end{minipage}



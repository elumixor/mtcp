
\section{Simulated Data and Event Weighting}

After formulating our problem as a supervised learning task, the next step is to find an appropriate way to tackle it.
To train a classifier, we need a set of labeled observations. In the context of particle physics, each observation is an
event and the label corresponds to the process that led to this event. However, obtaining these labeled observations is
not as straightforward as simply recording the events.

We need a model that describes the underlying physics. In our case, this is the \gls{sm}, including the branching
factors introduced before. The model is based on our understanding of particle physics and describes the probabilities
of different processes and outcomes.

To generate our labeled dataset, we use a tool called \gls{mc} simulations. \gls{mc} simulations are a way to generate
data that follows a given model. In essence, the simulation creates a large number of events, each following the
probabilistic rules of the model. We can then record these events and their corresponding labels (which process they
were created by).

The benefit of \gls{mc} simulations is that they allow us to create a large amount of data where we know the 'true' labels.
This data can be used to train and evaluate our classifier. However, it's important to note that while \gls{mc} simulations
are based on our best understanding of physics, they are not perfect. There are always uncertainties and potential
inaccuracies in the model, which can affect the generated data and the performance of our classifier.

To alleviate the difficulties of training on small dataset, more signal events are explicitly generated. This leads to
producing a different distribution than the one observed in the real data. To align the \gls{mc} simulated data with the real
data, it is necessary to apply event weights. Event weighting is a technique used to correct the simulated distributions
for known differences between the simulation and the real data.

% Event weights can account for a variety of effects:

% Luminosity: The total number of simulated events often does not correspond to the luminosity of the real data. Events
% are therefore weighted to correspond to the correct integrated luminosity. If $L_{data}$ is the integrated luminosity of
% the data and $L_{\gls{mc}}$ is the integrated luminosity of the \gls{mc} sample, then the luminosity weight for an event is $w_{L} =
%     L_{data}/L_{\gls{mc}}$.

% Cross-Section: Different processes have different probabilities (cross-sections) of occurring. The ratio of the
% cross-sections in data and simulation $\sigma_{data}/\sigma_{\gls{mc}}$ is used as a weight.

% Detector Effects: The detector response is not always perfectly simulated. Therefore, weights are applied to correct for
% known discrepancies in detector efficiencies and energy resolutions between the data and the simulation.

% Pileup: Pileup refers to additional proton-proton collisions that occur simultaneously with the event of interest.
% Pileup can significantly affect the event reconstruction. Pileup weights are used to match the pileup distribution in
% the data.

% Higher-order corrections: Theoretical predictions often include higher-order corrections, known as K-factors, to account
% for processes beyond the leading-order approximation used in the simulation.

% In mathematical terms, the total weight $w_{total}$ for an event can be written as the product of individual weights:

% \begin{equation}
%     w_{\text{total}} = w_{L} w_{\sigma} w_{\text{detector}} w_{\text{pileup}} w_{\text{K-factor}}
% \end{equation}

% Event weights are essential for ensuring that the simulated events accurately represent the conditions of the actual
% experiment. These weights allow us to make meaningful comparisons between the data and theoretical predictions, and are
% a crucial part of the analysis in high-energy physics.


% \input{chapters/3-data/v8/0-section.tex}


\subsection{Regions in Particle Physics}

In particle physics, especially in high energy experiments such as those conducted at the LHC, the parameter space is
vast. Identifying signals from noise or background events becomes a challenging task. To tackle this challenge, we
divide the parameter space into different regions, each serving a specific purpose. These regions include the \gls{sr},
\gls{cr}, and \gls{vr}.

\begin{itemize}
    \item \textbf{\gls{sr}} is where we expect the events of interest, the
          signal, to be most prevalent. It is defined by certain selection criteria that maximize the signal's prominence against
          the background. In our case, the signal refers to the \tth events.

    \item \textbf{\gls{cr}} is where we estimate the amount of background contamination
          present in the \gls{sr}. The \gls{cr} is characterized by negligible signal but a significant amount of background
          events, similar to what we expect in the \gls{sr}. By studying the \gls{cr}, we can understand and model the background in the \gls{sr}.

    \item \textbf{\gls{vr}} is used to test the reliability of our model predictions
          and the \gls{mc} simulations. The VR is typically chosen where neither the signal nor the background is expected to
          be particularly high or low. Any significant deviation of the observed data from our model predictions in the VR may
          indicate the presence of a new physics process or systematic errors in our model or simulation.
\end{itemize}

These regions are not arbitrarily defined but are carefully chosen based on detailed knowledge of the physics processes
involved and the detector's characteristics. They provide a robust framework for analyzing the data from high-energy
physics experiments and are crucial in our search for the \tth process events.

\subsection{\lss channel}
\label{sec:lss}

\input{figures/feynman/2lss1tau}

The work at CERN is often divided into different channels, with each channel focusing on a specific final state of
interest. These channels are orthogonal, meaning that each event can only belong to one channel, preventing any overlap
in the analysis.

One such channel, and the main focus of this thesis, is the two-lepton same-sign plus one tau (\lss) channel. This
specific final state arises from the decay of a top-antitop-Higgs (\tth) system, where one top quark decays to a W boson
and a b quark, with the W boson further decaying to a lepton and a neutrino. The other top quark also decays to a W
boson and a b quark, but in this case, the W boson decays into a pair of quarks. Lastly, the Higgs boson decays to a
pair of tau leptons, where one tau lepton decays into another lepton and two neutrinos, while the other tau lepton
decays to a pair of quarks and a neutrino. This complex series of decays results in a final state consisting of two
same-sign leptons and a hadronically decaying tau lepton, hence the name \lss channel (as shown in \autoref{fig:2lss1tau}).

The branching ratios for these decays are as follows:

\begin{align*}
    \text{Higgs boson to a pair of tau leptons: }\mathcal{B}(H \rightarrow \tau\tau)  & = 6.25\% \\
    \text{tau lepton to a W boson and a neutrino: }\mathcal{B}(\tau \rightarrow W\nu) & = 100\%  \\
    \text{top quark to a W boson and a b quark: }\mathcal{B}(t \rightarrow Wb)        & = 100\%  \\
    \text{W boson to a lepton and a neutrino: }\mathcal{B}(W \rightarrow l\nu)        & = 10\%   \\
    \text{W boson to a pair of quarks: }\mathcal{B}(W \rightarrow qq)                 & = 70\%
\end{align*}

Given that the theoretical cross-section for the \tth process is 500 fb and the luminosity is 140 fb$^{-1}$, the total
number of expected \tth events is approximately 70,000. When accounting for the aforementioned branching ratios, the
expected number of \tth events in the \lss final state is reduced to:

\begin{align*}
    N_{\text{\tth, \lss}} & = \sigma_{\text{\tth}} \cdot \mathcal{L} \cdot \mathcal{B}(H \rightarrow \tau\tau) \cdot \mathcal{B}(\tau \rightarrow W\nu) \cdot \mathcal{B}(t \rightarrow Wb) \cdot \mathcal{B}(W \rightarrow l\nu) \cdot \mathcal{B}(W \rightarrow qq) \\
                          & = 500 \text{ fb} \cdot 140 \text{ fb}^{-1} \cdot 6.25\% \cdot 100\% \cdot 100\% \cdot 10\% \cdot 70\% \approx 3.15
\end{align*}

\todo{Fix the above calculations, it's not correct}

% It should be noted, however, that this is a simplified calculation and does not consider various factors such as
% detector efficiencies and background events. Nevertheless, it provides a good estimate of the scale of the challenge we
% face in detecting the \tth process in the \lss channel.

As we apply the selection criteria to the data, we drop from about 8.9M to just 32K raw events. For the \tth, this
corresponds to 0.8M $\rightarrow$ 15K raw events. In terms of weighted events the transition is from 46K $\rightarrow$
32.72 weighted events across all the processes, and 523.42 $\rightarrow$ 12.22 weighted events for the \tth. The details
for all processes are shown on the \autoref{tab:class_distributions}.

\todo{highlight the numbers above}

\begin{figure}[htb]
    \centering
    \input{generated/num_events.tex}
    \captionof{table}{Comparison of the number of events before and after applying the selection criteria. For each
        process, we show the number of raw events and the number of weighted events.}
    \label{tab:class_distributions}
\end{figure}


The figures \autoref{fig:lep_pt_0} and \autoref{fig:lep_pt_1} show the distributions of the leading and
subleading leptons' transverse momenta inside inside the \gls{sr}. These types of plots are one of the most common
plots generated by the \trex\footnote{\trex is a framework for binned template profile likelihood fits heavily used at
    CERN. The documentation can be found at \url{https://trexfitter-docs.web.cern.ch/trexfitter-docs/}. The code can be
    found at \url{https://gitlab.cern.ch/TRExStats/TRExFitter}} software. These plots are often used to check the well
modelling of the variables. It is essential to make sure that the variables are well
modelled - events generated by the \gls{mc} agrees with the real data. Otherwise we can be training on the wrong data
and our classifier will not generalize well when it comes to the real data. More plots are presented in the
\appref{appendix:yields}.


\captionsetup[subfigure]{justification=centering}
\begin{figure}[htb]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/yields/lep-pt-0.pdf}
        \caption{Distribution of the transverse momentum of the leading lepton.}
        \label{fig:lep_pt_0}
    \end{subfigure}\hfill%
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/yields/lep-pt-1.pdf}
        \caption{Distribution of the transverse momentum of the subleading lepton.}
        \label{fig:lep_pt_1}
    \end{subfigure}
    \caption{Distributions of the transverse momentum of the leading and subleading leptons. The areas are crossed out
        because the events in these regions are blinded.}
\end{figure}

One might wonder why the plots are crossed out in some regions. This is because the events in these regions are blinded.
Blinding refers to hiding the data points in the bin if the relation of teh signal to background is larger than some
threshold. A threshold of $0.15$ is a common threshold to apply and that is what we have used throughout the thesis.
The reasoning behind the blinding is \todo{???}.


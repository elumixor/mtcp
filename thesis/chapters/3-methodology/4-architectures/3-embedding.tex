\subsection{Pre-processing and embedding}

In previous work, all the features were treated as continuous variables. Before feeding them to the network, they were
normalized to have zero mean and unit variance. This is essential for the training of deep neural networks, as it
prevents the gradients from exploding or vanishing.

However, this approach is far from optimal when working with the categorical features. We standard way of dealing with
categorical features is to ues embeddings. An embedding is a mapping from a discrete variable to a continuous vector
space. The embedding is learned during the training of the network. The embedding layer is essentially a lookup table,
where each row corresponds to a single category.

Furthermore, the dataset sometimes contains missing or invalid values for some samples. To properly handle those, we
introduce a separate category for them when the feature is categorical. For continuous features, we replace the missing
values with a learnable parameter.

The whole structure of the pre-processing layer is shown on the \autoref{fig:preprocessing}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{example-image-a}
    \caption{Pre-processing layer.}
    \label{fig:preprocessing}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/resnet_results.pdf}
    \caption{Resnet results.}
    \label{fig:resnet_results}
\end{figure}

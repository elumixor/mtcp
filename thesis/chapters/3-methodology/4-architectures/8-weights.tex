\section{Weights in the Training - Different Approaches}
\label{sec:weights}

As we described before, each event coming from the \gls{mc} simulation has an associated weight so that the distribution
of the simulated sample matches the distribution of the real data (see \autoref{sec:mc}). As described in the
\autoref{sec:evaluation}, weights should always be used during evaluation.

But what about the training? As described in \autoref{sec:cross-entropy}, the Cross-Entropy loss function allows to
assign a weight to each class, that would put more or less emphasis on it. In \cite{jan} authors have trained with
using the event weights as in \autoref{eq:weight-per-sample}, where, however, the weights were per-batch normalized to
sum to 1.

In our analysis we have experimented with different approaches, but we have found out that using them produced no
significant improvements. We have attempted several strategies:

\begin{enumerate}
    \item \textbf{No weights} - the weights were not used at all.
    \item \textbf{Per-sample weights} - the weights were used as in \cite{jan} (\autoref{eq:weight-per-sample}).
    \item \textbf{Address raw samples class imbalance} - the total number of \emph{raw} samples was divided by the
          number of raw \emph{samples} in each class. Then the weights were normalized (\autoref{eq:weight-per-class}).
    \item \textbf{Address weighted sample class imbalance} - the total number of \emph{weighted} samples was divided by
          the number of \emph{weighted} samples in each class. Then the weights were normalized
          (\autoref{eq:weight-per-class}).
    \item \textbf{Per-sample weights with raw class weights} - the weights were used as in \cite{jan}
          (\autoref{eq:weight-per-sample}), but additional weights applied to account for the class imbalance.
          Additional weights are calculated using the \emph{raw} number of samples.
    \item \textbf{Per-sample weights with weighted class weights} - the weights were used as in \cite{jan}
          (\autoref{eq:weight-per-sample}), but additional weights applied to account for the class imbalance.
          Additional weights are calculated using the \emph{weighted} number of samples.
\end{enumerate}

Unfortunately, any of the weighting methods did not produce any noticeable increase in performance. Training with
disregarding the weights completely turned out to be most effective. The results are summarized in
\autoref{tab:weights}.

\begin{table}[htb]
    \centering
    \begin{tabular}{ccc}
        \toprule
        Loss Function                      & Accuracy & AUC  \\
        \midrule
        Cross-Entropy                      & 0.85     & 0.91 \\
        Cross-Entropy with Class Weights   & 0.87     & 0.93 \\
        Sample-wise Weighted Cross-Entropy & 0.88     & 0.94 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different loss functions.}
    \label{tab:weights}
\end{table}

\section{Weights in the Training - Different Approaches}
\label{sec:weights}

As we described before, each event coming from the \gls{mc} simulation has an associated weight so that the distribution
of the simulated sample matches the distribution of the real data (see \autoref{sec:mc}). As described in the
\autoref{sec:evaluation}, weights should always be used during evaluation.

But what about the training? As described in \autoref{sec:cross-entropy}, the Cross-Entropy loss function allows to
assign a weight to each class, that would put more or less emphasis on it. In \cite{jan} authors have trained with
using the event weights as in \autoref{eq:weight-per-sample}, where, however, the weights were per-batch normalized to
sum to 1.

In our analysis we have experimented with different approaches, but we have found out that using them produced no
significant improvements. We have attempted several strategies:

\begin{enumerate}
      \item \textbf{No weights} - the weights were not used at all.
      \item \textbf{Per-sample weights} - the weights were used as in \cite{jan}.
      \item \textbf{Per-sample weights, not normalized} - the weights were used as in \cite{jan} but without the
            per-sample normalization (\autoref{eq:weight-per-sample}). \label{item:per-sample-not-normalized}
      \item \textbf{Raw class-imbalance weights} - the total number of \emph{raw}
            samples was divided by the number of raw \emph{samples} in each class. Then the weights were normalized
            (\autoref{eq:weight-per-class}).  \label{item:raw-class-imbalance}
      \item \textbf{Weighted class-imbalance weights} - the total number of \emph{weighted} samples was divided by
            the number of \emph{weighted} samples in each class. Then the weights were normalized
            (\autoref{eq:weight-per-class}). \label{item:weighted-class-imbalance}
      \item \textbf{Raw class-imbalance weights \& per-sample not normalized weights}:
            \autoref{item:per-sample-not-normalized} \& \autoref{item:raw-class-imbalance}
      \item \textbf{Weighted class-imbalance weights \& per-sample not normalized weights}:
            \autoref{item:per-sample-not-normalized} \& \autoref{item:weighted-class-imbalance}
\end{enumerate}

Results are shown on the \autoref{fig:weights}. Any of the weighting methods did not produce any substantial increase in
performance. Further investigation of the subject is suggested. We have decided to train without weights for simplicity.

\begin{figure}[htb]
      \centering
      \includegraphics[width=\textwidth]{figures/ml/weighting/results.pdf}
      \caption[Comparison of different weighting strategies.]
      {Comparison of different weighting strategies. \gls{ftt} with 2 blocks was used on the standard training set
            \ttrn.}
      \label{fig:weights}
\end{figure}

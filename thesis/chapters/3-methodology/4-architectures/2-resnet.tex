\subsection[Residual Neural Network]{\acrfull{resnet}}
\label{sec:resnet}

We compare the staged network to a slightly improved version of the \gls{mlp} that introduces residual/skip
connections between the layers (\autoref{fig:resnet_architecture}) proposed by \cite{resnet}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/resnet.pdf}
    \caption[ResNet architecture]{\gls{resnet} architecture.}
    \label{fig:resnet_architecture}
\end{figure}

These connections improve the training of deep neural networks, as the gradient can flow unimpeded back to the first
layers. This helps to address the vanishing gradient problem. We can formalize this as in
\cite{ft-transformer}:


{\fontsize{11}{10}\selectfont
\begin{align}
    \texttt{ResNet(x)}      & = \texttt{Prediction}(\texttt{ResNetBlock}(\dots(\texttt{ResNetBlock}(\Linear(\Embed(x)))))) \\
    \texttt{ResNetBlock(x)} & = x + \Dropout(\Linear(\LayerNorm(\sigma(x))))                                               \\
    \texttt{Prediction}(x)  & = \Linear(\LayerNorm(\sigma(x)))
\end{align}
}


Because of residual connections, \glspl{resnet} are very fast to train, and are more sample efficient than \glspl{mlp}.
While keeping the number of trainable parameters the same, we have observed that deeper networks perform better than
wider ones.  Although wide \glspl{nn} are fast to train, they are extremely prone to overfitting, as the wide layers
close to the inputs essentially memorize the training data.


We introduce a few other changes to the training procedure:

\begin{enumerate}
    \item We use an \acrfull{adamw} optimizer \cite{adamw}.
    \item We use a \GELU activation \cite{gelu}.
    \item We introduce \LayerNorm \cite{layernorm} layers before each \Linear layer.
\end{enumerate}

\section{Architectures and Optimization Techniques}

This section covers the classifier architectures and optimization techniques we employed in our research. The primary
models used for this classification task are simple feed-forward neural networks with residual connections (ResNets) and
the Feature Tokenizer-Transformer (FT-Transformer). The former serves as a baseline, being merely Multi-Layer
Perceptrons (MLPs) enhanced with skip connections. On the other hand, the FT-Transformer is an adaptation of the
original Transformer model, designed to handle tabular data. The process of training these models and the optimization
techniques used to improve their performance will be detailed in the subsequent sections.

\input{chapters/3-methodology/4-architectures/1-previous-works}
\input{chapters/3-methodology/4-architectures/2-resnet}
\input{chapters/3-methodology/4-architectures/3-embedding}
\input{chapters/3-methodology/4-architectures/4-transformer}
\input{chapters/3-methodology/4-architectures/5-dropout}
\input{chapters/3-methodology/4-architectures/6-dropping-cuts}
\input{chapters/3-methodology/4-architectures/7-reduced-training-set}
\input{chapters/3-methodology/4-architectures/8-weights}
\input{chapters/3-methodology/4-architectures/9-classes-fine-tuning}